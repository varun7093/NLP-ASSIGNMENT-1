{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my6eAXnO0ZAz",
        "outputId": "9eb0918d-42d2-4604-f88d-ea9867b58908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Q2. Tokenization Assignment ===\n",
            "\n",
            "Original Paragraph:\n",
            " The quick brown fox jumps over the lazy dog.\n",
            "It’s a classic sentence used in typing practice.\n",
            "However, many people don’t realize its history. \n",
            "\n",
            "1. Naïve space-based tokens:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.', 'It’s', 'a', 'classic', 'sentence', 'used', 'in', 'typing', 'practice.', 'However,', 'many', 'people', 'don’t', 'realize', 'its', 'history.'] \n",
            "\n",
            "1. Manually corrected tokens:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', '’s', 'a', 'classic', 'sentence', 'used', 'in', 'typing', 'practice', '.', 'However', ',', 'many', 'people', 'don’t', 'realize', 'its', 'history', '.'] \n",
            "\n",
            "2. spaCy tool tokens:\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', '\\n', 'It', '’s', 'a', 'classic', 'sentence', 'used', 'in', 'typing', 'practice', '.', '\\n', 'However', ',', 'many', 'people', 'do', 'n’t', 'realize', 'its', 'history', '.'] \n",
            "\n",
            "2. Differences (manual vs spaCy):\n",
            "  Manual: It   |   spaCy: \n",
            "\n",
            "  Manual: ’s   |   spaCy: It\n",
            "  Manual: a   |   spaCy: ’s\n",
            "  Manual: classic   |   spaCy: a\n",
            "  Manual: sentence   |   spaCy: classic\n",
            "  Manual: used   |   spaCy: sentence\n",
            "  Manual: in   |   spaCy: used\n",
            "  Manual: typing   |   spaCy: in\n",
            "  Manual: practice   |   spaCy: typing\n",
            "  Manual: .   |   spaCy: practice\n",
            "  Manual: However   |   spaCy: .\n",
            "  Manual: ,   |   spaCy: \n",
            "\n",
            "  Manual: many   |   spaCy: However\n",
            "  Manual: people   |   spaCy: ,\n",
            "  Manual: don’t   |   spaCy: many\n",
            "  Manual: realize   |   spaCy: people\n",
            "  Manual: its   |   spaCy: do\n",
            "  Manual: history   |   spaCy: n’t\n",
            "  Manual: .   |   spaCy: realize\n",
            "  Extra spaCy tokens: ['its', 'history', '.']\n",
            "\n",
            "3. Multiword Expressions (MWEs):\n",
            "- 'New York City' should be treated as one token (meaning is lost if split).\n",
            "- 'kick the bucket' should be treated as one token (meaning is lost if split).\n",
            "- 'high school' should be treated as one token (meaning is lost if split).\n",
            "\n",
            "\n",
            "4. Reflection:\n",
            "The hardest part of tokenization in English was handling contractions like \"don’t\" or \"it’s\",\n",
            "since they may be split differently depending on the tool. Punctuation also introduces\n",
            "challenges, because naïve space-based methods attach punctuation marks to words. Compared\n",
            "to English, morphologically rich languages (like Turkish or Hindi) are harder, since suffixes\n",
            "and word forms require morphological analysis. Multiword expressions add another difficulty\n",
            "because their meaning disappears when split. Tools like spaCy generally perform well,\n",
            "but sometimes make tokenization choices that differ from manual expectations.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Paragraph (3–4 sentences)\n",
        "text = \"\"\"The quick brown fox jumps over the lazy dog.\n",
        "It’s a classic sentence used in typing practice.\n",
        "However, many people don’t realize its history.\"\"\"\n",
        "\n",
        "print(\"=== Q2. Tokenization Assignment ===\\n\")\n",
        "print(\"Original Paragraph:\\n\", text, \"\\n\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. Naïve space-based tokenization\n",
        "naive_tokens = text.split()\n",
        "print(\"1. Naïve space-based tokens:\")\n",
        "print(naive_tokens, \"\\n\")\n",
        "\n",
        "# Manually corrected tokens\n",
        "manual_tokens = [\n",
        "    \"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\",\n",
        "    \"It\", \"’s\", \"a\", \"classic\", \"sentence\", \"used\", \"in\", \"typing\", \"practice\", \".\",\n",
        "    \"However\", \",\", \"many\", \"people\", \"don’t\", \"realize\", \"its\", \"history\", \".\"\n",
        "]\n",
        "print(\"1. Manually corrected tokens:\")\n",
        "print(manual_tokens, \"\\n\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. Compare with spaCy tool\n",
        "doc = nlp(text)\n",
        "tool_tokens = [token.text for token in doc]\n",
        "print(\"2. spaCy tool tokens:\")\n",
        "print(tool_tokens, \"\\n\")\n",
        "\n",
        "# Show differences clearly\n",
        "print(\"2. Differences (manual vs spaCy):\")\n",
        "i, j = 0, 0\n",
        "while i < len(manual_tokens) and j < len(tool_tokens):\n",
        "    if manual_tokens[i] == tool_tokens[j]:\n",
        "        i += 1\n",
        "        j += 1\n",
        "    else:\n",
        "        print(f\"  Manual: {manual_tokens[i]}   |   spaCy: {tool_tokens[j]}\")\n",
        "        # advance both, because it's a misalignment\n",
        "        i += 1\n",
        "        j += 1\n",
        "# If extra tokens remain\n",
        "if i < len(manual_tokens):\n",
        "    print(\"  Extra manual tokens:\", manual_tokens[i:])\n",
        "if j < len(tool_tokens):\n",
        "    print(\"  Extra spaCy tokens:\", tool_tokens[j:])\n",
        "print()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3. Multiword Expressions (MWEs)\n",
        "MWEs = [\n",
        "    \"New York City\",   # place name\n",
        "    \"kick the bucket\", # idiom\n",
        "    \"high school\"      # fixed phrase\n",
        "]\n",
        "print(\"3. Multiword Expressions (MWEs):\")\n",
        "for expr in MWEs:\n",
        "    print(f\"- '{expr}' should be treated as one token (meaning is lost if split).\")\n",
        "print()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 4. Reflection\n",
        "reflection = \"\"\"\n",
        "4. Reflection:\n",
        "The hardest part of tokenization in English was handling contractions like \"don’t\" or \"it’s\",\n",
        "since they may be split differently depending on the tool. Punctuation also introduces\n",
        "challenges, because naïve space-based methods attach punctuation marks to words. Compared\n",
        "to English, morphologically rich languages (like Turkish or Hindi) are harder, since suffixes\n",
        "and word forms require morphological analysis. Multiword expressions add another difficulty\n",
        "because their meaning disappears when split. Tools like spaCy generally perform well,\n",
        "but sometimes make tokenization choices that differ from manual expectations.\n",
        "\"\"\"\n",
        "print(reflection)"
      ]
    }
  ]
}